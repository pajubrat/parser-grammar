Linear phase parser
Pauli Brattico
2020

The linear phase parser analyses language by simulating human language comprehension. The program was originally created at IUSS, Pavia, in a research project "ProGraM-PC: A Processing-friendly Grammatical Model for Parsing and predicting on-line Complexity". The system is a tool for linguistics and language comprehension research in that it allows one to model the comprehension of language (semantic interpretation, grammaticality judgments, etc.) at any level of detail desired and under completely formal and rigorous regimen.

The program reads sentences (linear sequences of unannotated words in any language) as input and figures out their meaning by applying a series of language comprehension principles of the human brain. If the input is  ungrammatical, semantic interpretation is not generated and the input is judged ungrammatical by the model. The model provides also a large collection of quantitative data, such as number and timing (ms from stimulus onset) of all of its internation operations, which can be used to assess cognitive load and perceived "unnaturalness" of the input sentence. I am working on a system that allows one to map this data further to brain localization.

To run the program, you must have Python installed and configured at your local machine. Quantitative analysis requires that you have the pandas library. After cloning the repository, type 'python lpparse' into command prompt inside the main folder where the package is installed. This command should process all sentences from a test corpus file. You should see the output in the console. To generate phrase structure images, you need the pyglet package at the local machine. The location of the test corpus file and other such information is written in the 'config.txt' file that is located in the main directory. To process different file, change the folder and file information inside config.txt. All output is generated into the study folder defined in config.txt.

The current version offers two additional use cases. It is now possible to run several studies at once. To do so, type 'python lpparse multi' into the command prompt. This will execure a script from 'multistudy.py' which contains all the studies that the user wants to execute, as specified in that module. It is now also possible to obtain and analyse quantitative performance data. To so so, type 'python lpparse diagnostics' into the command prompt. This executes a function from the 'diagnostics.py' module that reads data from study folders, as specified in that module, and analyses it. You can present statistical summaries, create plots and do other things. This module uses the pandas library to work with the data. It does not do anything sophisticated at present; it is just the framework to do these analyses.

The algorithm produces the following outputs: (1) a list of input sentences and whether the model judged then grammatical or ungrammatical (...grammaticality_judgments.txt); (2) more detailed phrase structure analyses and semantic interpretation (...results.txt); (3) a detailed csv data concerning the number of operations (...resources.txt) that can be processed easily for example with Python pandas and matplotlib; (4) a detailed log of virtually every operation that occurred during the processing (...log.txt); (5) a detailed list of the order of all operations and their timings (ms)(...resource_sequence). (6) Phrase structure images are generated inside /phrase structure images if the study is configured to make them. In a real empirical study the focus is always first on observational adequacy, i.e. whether the algorithm replicates human behavior. Once the model reaches that criterion, we look at the rest of the output and make necessary adjustments without breaking observational adequacy.

Should the user want to examine the derivation of only one sentence,  that sentence can be prefaced with symbol % in the test corpus file. Then only that sentence will be processed. If the user wants to process a batch of sentences, they can be selected by writing =START= and =STOP= at the beginning of the respective starting and stopping lines, and the algorithm will process every sentence between.

In the old versions, if the user wanted to execute the algorithm with different parameteres, the corresponding change had to be carried to the actual code. This is no longer the case. Many of the internal parameters can be controlled by using a study-specific external configuration file. These files are called 'config_study.txt' and they are located inside the corresponding study folders. The reason for these changes is that it is now possible to run the algorithm automatically with several parameters (by using the multistudy functionality) and compare the results. The parameters that are currently not available have not been documented, though. They will be documented for the version 7x.

In sum, the program uses the following architecture: when 'python lpparse' is executed, the program reads 'config.txt' from the main directory and targets a study defined there. It then reads the 'study_config.txt' from the targeted study folder and configures its internal parameters. Once this is done, it will process the test corpus as specified in the config.txt, and procudes the output to the study folder. A multistudy does this process for several studies at once.