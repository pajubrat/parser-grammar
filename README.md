Linear phase parser
Pauli Brattico
2020

The linear phase parser analyses language by simulating human language comprehension. The program was originally created at IUSS, Pavia, in a research project "ProGraM-PC: A Processing-friendly Grammatical Model for Parsing and predicting on-line Complexity". Documentation is in the /docs folder, and research paper preprints are in /docs/Article preprints and supplementary documents. The system, which is essentially just a computational model of human language comprehension system in the brain, is a tool for linguistics and language comprehension research in that it allows one to model the comprehension of language (semantic interpretation, grammaticality judgments, etc.) at any level of detail desired and under completely formal and rigorous regimen.

The program reads sentences (linear sequences of unannotated words in any language) as input and figures out their meaning by applying a series of language comprehension principles of the human brain. If the input is judged ungrammatical, semantic interpretation is not generated and the input is judged ungrammatical. The model provides also a large collection of quantitative data, such as number and timing (ms from stimulus onset) of all of its internation operations, which can be used to assess cognitive load and perceived "unnaturalness" of the input sentence. I am working currently on system that allows me to map this data further with brain localization. 

To run the program, you must have python installed and configured at your local machine. After cloning the repository, type 'python lpparse' into command prompt inside the main folder where the package is installed. This command should process all sentences from a test corpus file (that is, if the latest version you have cloned is functional, usually it is). You should see the output in the console. To generate phrase structure images, you need the pyglet package at the local machine. To install it, write 'pip install pyglet' in the command prompt or see the pyglet installation instructions. 

The test corpus file, which contains all the sentences processed by the algorithm, is provided inside /language data working directory/xxx, where 'xxx' refers to an individual study folder. This is done so as to preserve a copy of the data used in each published/submitted study, and to make it possible to work with several studies at the same time. The output will be generated to the same directory. All this information is provided in the config.txt file that is in the main directory. Thus, if config.txt contains lines "test_corpus_file:	linear_phase_theory_corpus.txt" and 
"study_folder: study-6-linear-phase-theory", then the former will contain the sentences that are processed and the latter its folder; all output will appear inside the same folder. At present, the lexicon (which is also provided as external files) is provided in the /language data working directory and is therefore the same for each study, as I am currently working with a cumulative model that is not allowed to break previous results. These files should be copied however to the study subfolder in connection with each study to preserve a snapshot of the lexicon and for replication purposes. Each word in the test corpus file should appear in the lexicon.

The output is generated into the study folder defined in 'config.txt' hosting the test corpus as well. The following outputs are generated: a list of input sentences and whether the model judged then grammatical or ungrammatical (...grammaticality_judgments.txt); more detailed phrase structure analyses and semantic interpretation (...results.txt); a detailed csv data concerning the number of operations (...resources.txt) that can be processed easily for example with Python pandas and matplotlib; a detailed log of virtually every operation that occurred during the processing (...log.txt); a detailed list of the order of all operations and their timings (ms)(...resource_sequence). Phrase structure images are generated inside /phrase structure images if the study is configured to make them. In a real empirical study the focus is alwyas first on observational adequacy, i.e. whether the algorithm replicates human behavior. Once the model reaches that criterion, we look at the rest of the output and make necessary adjustments without breaking observational adequacy.

Should the user want to examine the derivation of only one sentence,  that sentence can be prefaced with symbol % in the test corpus file. Then only that sentence will be processed. If the user wants to process a batch of sentences, they can be selected by writing =START= and =STOP= at the beginning of the respective starting and stopping lines, and the algorithm will process every sentence between.
